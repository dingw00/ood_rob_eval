{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook aims to visualize the performance and robustness metrics of different model variants and OoD detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from adjustText import adjust_text\n",
    "from matplotlib.lines import Line2D\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "from utils.visualize import *\n",
    "\n",
    "# Load configs: benchmarks, model variants, OoD datasets and save directory.\n",
    "with open('config.yaml', 'r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "    \n",
    "score_functions = configs[\"score_functions\"]\n",
    "rand_seed = configs[\"rand_seed\"]\n",
    "\n",
    "# Define the order of perturbation functions and model variants in visualizations.\n",
    "perturb_function_sorter = configs[\"perturb_functions\"] + [\"average\"]\n",
    "variant_sorter = [\"NT\", \"DA\", \"AT\", \"PAT\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Performance - Robustness Scatter Plot\n",
    "The visualization of performance and robustness metrics is in the format of scatter plot. All the statistics can be inspected in the tables generated in `results/eval/performance/` and `results/eval/robustness/` folders. The generated figures are saved in `results/eval/performance_robustness/` folder.\n",
    "\n",
    "Specifically, we visualize the performance and robustness metrics in the following 2 types of figures:\n",
    "- Model accuracy $(\\uparrow)$ - MAE rate $(\\downarrow)$ plot\n",
    "- OoD detectors' FPR95 $(\\downarrow)$ - DAE rate $(\\downarrow)$ plot.\n",
    "\n",
    "\\* The $\\uparrow$ and $\\downarrow$ arrows indicate better performance/robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"results\", \"eval\", \"performance_robustness\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load DNN model and OoD detectors' performance metrics.\n",
    "performance_dir = os.path.join(\"results\", \"eval\", \"performance\")\n",
    "robustness_dir = os.path.join(\"results\", \"eval\", \"robustness\")\n",
    "df_model_perf = None\n",
    "df_detector_perf = None\n",
    "file_path = os.path.join(performance_dir, \"model_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_model_perf = pd.read_csv(file_path).copy()\n",
    "    df_model_perf = df_model_perf.set_index([\"benchmark\", \"model\"])\n",
    "file_path = os.path.join(performance_dir, \"detector_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_detector_perf = pd.read_csv(file_path).copy()\n",
    "    df_detector_perf = df_detector_perf.set_index([\"benchmark\", \"model\", \"dataset\"])\n",
    "assert (df_model_perf is not None) and (df_detector_perf is not None)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    \n",
    "    # Load the MAE/DAE rate table on ID dataset.\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_id_local_mae_dae_rate_mean_std.csv\")\n",
    "    df_id = None\n",
    "    if os.path.exists(file_path):\n",
    "        df_id = pd.read_csv(file_path).copy()   \n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].astype(\"category\")\n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].astype(\"category\")\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "        df_id = df_id.sort_values(by=[\"model\", \"variant\"])\n",
    "\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "\n",
    "    # Load the DAE rate table on OoD dataset.\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_ood_local_dae_rate_mean_std.csv\")\n",
    "    df_ood = None\n",
    "    if os.path.exists(file_path):    \n",
    "        df_ood = pd.read_csv(file_path).copy()   \n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].astype(\"category\")\n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].astype(\"category\")\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "        df_ood = df_ood.sort_values(by=[\"model\", \"variant\"])\n",
    "\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "    \n",
    "    models = list(configs[\"benchmark\"][benchmark][\"model\"].keys())\n",
    "    ncols = len(models)\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=1, figsize=(ncols*12, 8), layout=\"constrained\")\n",
    "    if ncols == 1:\n",
    "        axes = [axes]\n",
    "    markers = [\"o\", \"^\", \"s\", \"d\"]\n",
    "    colors_id = [\"lightsalmon\", \"indianred\", \"red\", \"darkred\"]\n",
    "    colors_ood = [\"lightskyblue\", \"royalblue\", \"blue\", \"midnightblue\"]\n",
    "\n",
    "    # DAE rate of different score functions.\n",
    "    for ax, model_name in zip(axes, models):\n",
    "        # Select relevant performance data (FPR95)\n",
    "        if (benchmark, model_name, \"average\") not in df_detector_perf.index:\n",
    "            continue\n",
    "        data_perf = df_detector_perf.loc[(benchmark, model_name, \"average\")].round(3).reset_index(drop=True)\n",
    "\n",
    "        # Process robustness data (DAE rate)\n",
    "        # Average ID/OoD DAE rate of different score functions.\n",
    "        if (df_id is not None) and (df_ood is not None):\n",
    "            data_id_dae = df_id[(df_id[\"model\"]==model_name) & (df_id[\"perturb_function\"]==\"average\")].copy()              \n",
    "            dae_cols = [col for col in data_id_dae.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            data_id_dae = data_id_dae.melt(id_vars=[\"variant\"], value_vars=dae_cols, var_name=\"score_function\", value_name=\"id_dae_rate\")\n",
    "            data_id_dae[\"score_function\"] = data_id_dae[\"score_function\"].apply(lambda s: s.replace(\"_mean\",\"\").replace(\"dae_\",\"\")).copy()\n",
    "            \n",
    "            data_ood_dae = df_ood[(df_ood[\"model\"]==model_name) & (df_ood[\"perturb_function\"]==\"average\") & (df_ood[\"dataset\"]==\"average\")].copy()\n",
    "            dae_cols = [col for col in data_ood_dae.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            data_ood_dae = data_ood_dae.melt(id_vars=[\"variant\"], value_vars=dae_cols, var_name=\"score_function\", value_name=\"ood_dae_rate\")\n",
    "            data_ood_dae[\"score_function\"] = data_ood_dae[\"score_function\"].apply(lambda s: s.replace(\"_mean\",\"\").replace(\"dae_\",\"\")).copy()\n",
    "            \n",
    "            data_perf_dae = pd.merge(data_perf, data_id_dae, on=[\"variant\", \"score_function\"]).copy()\n",
    "            data_perf_dae = pd.merge(data_perf_dae, data_ood_dae, on=[\"variant\", \"score_function\"]).copy()\n",
    "            data_perf_dae[\"score_function\"] = data_perf_dae[\"score_function\"].apply(lambda s: s.replace(\"Mahalanobis\", \"MD\").replace(\"KLMatching\", \"KLM\").replace(\"EnergyBased\", \"EB\").replace(\"MaxLogit\", \"ML\")).copy()\n",
    "\n",
    "            # DAE rate - FPR95 plot\n",
    "            variants = data_perf_dae[\"variant\"].unique()\n",
    "            legend_elements = [] # Patch(facecolor=colors_id[2], edgecolor='w', label='ID'), Patch(facecolor=colors_ood[2], edgecolor='w', label='OoD')\n",
    "            annos = []\n",
    "            for v_i, variant in enumerate(variants):\n",
    "                data_i = data_perf_dae[data_perf_dae[\"variant\"]==variant].copy()\n",
    "                x = data_i[\"FPR95\"].to_numpy()\n",
    "                y1 = data_i[\"id_dae_rate\"].to_numpy()\n",
    "                y2 = data_i[\"ood_dae_rate\"].to_numpy()\n",
    "                p = data_i[\"score_function\"].to_numpy()\n",
    "                sns.scatterplot(data=data_i, x=\"FPR95\", y=\"id_dae_rate\", ax=ax, color=colors_id[v_i], marker=markers[v_i])\n",
    "                sns.scatterplot(data=data_i, x=\"FPR95\", y=\"ood_dae_rate\", ax=ax, color=colors_ood[v_i], marker=markers[v_i])\n",
    "                legend_elements.append(Line2D([0], [0], marker=markers[v_i], label=variant, markerfacecolor=colors_id[v_i], markersize=10, color=\"w\"))\n",
    "                legend_elements.append(Line2D([0], [0], marker=markers[v_i], label=variant, markerfacecolor=colors_ood[v_i], markersize=10, color=\"w\"))\n",
    "\n",
    "\n",
    "                annos1_ = [ax.text(x[i], y1[i]+0.5, p[i], color=colors_id[v_i]) for i in range(len(x)) if (str(x[i])!=\"nan\") and (str(y1[i])!=\"nan\")]\n",
    "                annos2_ = [ax.text(x[i], y2[i]+0.5, p[i], color=colors_ood[v_i]) for i in range(len(x)) if (str(x[i])!=\"nan\") and (str(y1[i])!=\"nan\")]\n",
    "                annos += annos1_ + annos2_\n",
    "                \n",
    "            adjust_text(annos, ax=ax, avoid_self=False, explode_radius=0.1, expand=(1.2,1.2), force_text=(0.25,0.25))\n",
    "            ax.set_xlabel(\"FPR95 (%) \\u2193\")\n",
    "            ax.set_ylabel(\"DAE rate (%) \\u2193\")\n",
    "            ax.set_title(f\"Model={model_name}\")\n",
    "            legend_elements = legend_elements[::2] + legend_elements[1::2]\n",
    "        \n",
    "    plt.legend(handles=legend_elements, loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), ncols=2, title=\"ID                OoD\")\n",
    "    plt.suptitle(f\"FPR95 - DAE rate\\nBenchmark={benchmark}\")\n",
    "    plt.savefig(os.path.join(save_dir, f\"{benchmark}_fpr95_dae_rate.png\"))\n",
    "    plt.show()\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"results\", \"eval\", \"performance_robustness\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load DNN model and OoD detectors' performance metrics.\n",
    "performance_dir = os.path.join(\"results\", \"eval\", \"performance\")\n",
    "robustness_dir = os.path.join(\"results\", \"eval\", \"robustness\")\n",
    "df_model_perf = None\n",
    "df_detector_perf = None\n",
    "file_path = os.path.join(performance_dir, \"model_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_model_perf = pd.read_csv(file_path).copy()\n",
    "    df_model_perf = df_model_perf.set_index([\"benchmark\", \"model\"])\n",
    "file_path = os.path.join(performance_dir, \"detector_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_detector_perf = pd.read_csv(file_path).copy()\n",
    "    df_detector_perf = df_detector_perf.set_index([\"benchmark\", \"model\", \"dataset\"])\n",
    "assert (df_model_perf is not None) and (df_detector_perf is not None)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    \n",
    "    # Load the MAE/DAE rate table on ID dataset.\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_id_local_mae_dae_rate_mean_std.csv\")\n",
    "    df_id = None\n",
    "    if os.path.exists(file_path):\n",
    "        df_id = pd.read_csv(file_path).copy()   \n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].astype(\"category\")\n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].astype(\"category\")\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "        df_id = df_id.sort_values(by=[\"model\", \"variant\"])\n",
    "\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "\n",
    "    # Load the DAE rate table on OoD dataset.\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_ood_local_dae_rate_mean_std.csv\")\n",
    "    df_ood = None\n",
    "    if os.path.exists(file_path):    \n",
    "        df_ood = pd.read_csv(file_path).copy()   \n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].astype(\"category\")\n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].astype(\"category\")\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "        df_ood = df_ood.sort_values(by=[\"model\", \"variant\"])\n",
    "\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "\n",
    "    # DAE rate of different score functions.\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        # Select relevant performance data (FPR95)\n",
    "        if (benchmark, model_name, \"average\") not in df_detector_perf.index:\n",
    "            continue\n",
    "        data_perf = df_detector_perf.loc[(benchmark, model_name, \"average\")].round(3).reset_index(drop=True)\n",
    "        \n",
    "        # Process robustness data (DAE rate)\n",
    "        # Average ID/OoD DAE rate of different score functions.\n",
    "        if (df_id is not None) and (df_ood is not None):\n",
    "            data_id_dae = df_id[(df_id[\"model\"]==model_name) & (df_id[\"perturb_function\"]==\"average\")].copy()              \n",
    "            dae_cols = [col for col in data_id_dae.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            data_id_dae = data_id_dae.melt(id_vars=[\"variant\"], value_vars=dae_cols, var_name=\"score_function\", value_name=\"id_dae_rate\")\n",
    "            data_id_dae[\"score_function\"] = data_id_dae[\"score_function\"].apply(lambda s: s.replace(\"_mean\",\"\").replace(\"dae_\",\"\")).copy()\n",
    "            \n",
    "            data_ood_dae = df_ood[(df_ood[\"model\"]==model_name) & (df_ood[\"perturb_function\"]==\"average\") & (df_ood[\"dataset\"]==\"average\")].copy()\n",
    "            dae_cols = [col for col in data_ood_dae.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            data_ood_dae = data_ood_dae.melt(id_vars=[\"variant\"], value_vars=dae_cols, var_name=\"score_function\", value_name=\"ood_dae_rate\")\n",
    "            data_ood_dae[\"score_function\"] = data_ood_dae[\"score_function\"].apply(lambda s: s.replace(\"_mean\",\"\").replace(\"dae_\",\"\")).copy()\n",
    "            \n",
    "\n",
    "            data_perf_dae = pd.merge(data_perf, data_id_dae, on=[\"variant\", \"score_function\"]).copy()\n",
    "            data_perf_dae = pd.merge(data_perf_dae, data_ood_dae, on=[\"variant\", \"score_function\"]).copy()\n",
    "            data_perf_dae[\"score_function\"] = data_perf_dae[\"score_function\"].apply(lambda s: s.replace(\"Mahalanobis\", \"MD\").replace(\"KLMatching\", \"KLM\").replace(\"EnergyBased\", \"EB\").replace(\"MaxLogit\", \"ML\")).copy()\n",
    "            \n",
    "            # DAE rate - FPR95 plot\n",
    "            variants = sorted(set(data_perf_dae[\"variant\"]), key=list(data_perf_dae[\"variant\"]).index)\n",
    "            ncols = len(variants)\n",
    "            nrows = 1\n",
    "            fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols*6, nrows*6), layout=\"constrained\", sharey=True)\n",
    "            if ncols > 1 or nrows > 1:\n",
    "                axes = axes.flatten()\n",
    "            else:\n",
    "                axes = [axes]\n",
    "\n",
    "            for a_i, ax in enumerate(axes):\n",
    "                if a_i < len(variants):\n",
    "                    variant = variants[a_i]\n",
    "                    data_i = data_perf_dae[data_perf_dae[\"variant\"]==variant].copy()\n",
    "                    x = data_i[\"FPR95\"].to_numpy()\n",
    "                    y1 = data_i[\"id_dae_rate\"].to_numpy()\n",
    "                    y2 = data_i[\"ood_dae_rate\"].to_numpy()\n",
    "                    p = data_i[\"score_function\"].to_numpy()\n",
    "                    sns.scatterplot(data=data_i, x=\"FPR95\", y=\"id_dae_rate\", ax=ax, color=\"red\", label=\"ID\", marker=\"v\")\n",
    "                    sns.scatterplot(data=data_i, x=\"FPR95\", y=\"ood_dae_rate\", ax=ax, color=\"blue\", label=\"OOD\", marker=\"^\")\n",
    "\n",
    "                    annos1 = [ax.text(x[i], y1[i], p[i], color=[0.5, 0, 0]) for i in range(len(x)) if (str(x[i])!=\"nan\") and (str(y1[i])!=\"nan\")]\n",
    "                    annos2 = [ax.text(x[i], y2[i], p[i], color=[0, 0, 0.5]) for i in range(len(x)) if (str(x[i])!=\"nan\") and (str(y1[i])!=\"nan\")]\n",
    "                    adjust_text(annos1+annos2, ax=ax, avoid_self=False, explode_radius=0.1, expand=(1.2,1.2), force_text=(0.25,0.25))\n",
    "\n",
    "                    ax.set_xlabel(\"FPR95 (%) \\u2193\")\n",
    "                    ax.set_ylabel(\"DAE rate (%) \\u2193\")\n",
    "                    ax.set_title(f\"Variant={variant}\")\n",
    "                    ax.legend()\n",
    "                    \n",
    "                else:\n",
    "                    ax.grid(\"off\")\n",
    "\n",
    "            plt.suptitle(f\"FPR95 - DAE rate\\nBenchmark={benchmark}, model={model_name}\")\n",
    "            plt.savefig(os.path.join(save_dir, f\"{benchmark}_{model_name}_fpr95_dae_rate.png\"))\n",
    "            plt.show()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "    # MAE rate of different model variants.\n",
    "    # Select relevant performance data (accuracy)\n",
    "    if benchmark not in df_model_perf.index:\n",
    "        continue\n",
    "    data_perf = df_model_perf.loc[benchmark].reset_index()\n",
    "    # Process robustness data (MAE rate)\n",
    "    if (df_id is not None):\n",
    "        data_id_mae = df_id[df_id[\"perturb_function\"]==\"average\"].copy()\n",
    "        data_id_mae = data_id_mae.melt(id_vars=[\"model\", \"variant\"], value_vars=\"mae_mean\", var_name=\"score_function\", value_name=\"mae_rate\")\n",
    "        data_perf_mae = pd.merge(data_perf, data_id_mae, on=[\"model\", \"variant\"]).copy()\n",
    "\n",
    "        # Accuracy - MAE rate plot\n",
    "        fig, ax = plt.subplots(figsize=(6, 4), layout=\"constrained\")\n",
    "        sns.scatterplot(data=data_perf_mae.copy(), x=\"accuracy\", y=\"mae_rate\", hue=\"model\", style=\"variant\", ax=ax)\n",
    "        ax.set_xlabel(\"Accuracy (%) \\u2191\")\n",
    "        ax.set_ylabel(\"MAE rate (%) \\u2193\")\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "        plt.suptitle(f\"Model accuracy - MAE rate\\nBenchmark={benchmark}\")\n",
    "        plt.savefig(os.path.join(save_dir, f\"{benchmark}_accuracy_mae_rate.png\"))\n",
    "        plt.show()\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Local MAE/DAE rate decomposition.\n",
    "The visualization of local MAE/DAE rate is in the format of radar plots. All the statistics data can be inspected in the tables saved in `results/eval/robustness/` folder.\n",
    "\n",
    "Specifically, we demonstrate the local MAE/DAE rate in the following 4 types of figures:\n",
    "- MAE rate\n",
    "    - Different model variants (color) under various perturbations (theta).\n",
    "- ID DAE rate\n",
    "    - Different model variants (color) under various perturbations (theta).\n",
    "- OoD DAE rate\n",
    "    - Different model variants (color) under various perturbations (theta).\n",
    "    - Different model variants (color) tested on various OoD datasets (theta).\n",
    "    - For each model variant, we also demonstrate the details of DAE rate on different OoD datasets (color) under various perturbations (theta).\n",
    "- Compare the relationship among the three robustness metrics for each model variant\n",
    "    - MAE rate, ID DAE rate, and OoD DAE rate (color).\n",
    "    - Under different perturbations (theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the refresh flag to True to regenerate all the figures.\n",
    "refresh = True\n",
    "\n",
    "# Load DNN model and OoD detectors' performance metrics.\n",
    "performance_dir = os.path.join(\"results\", \"eval\", \"performance\")\n",
    "robustness_dir = os.path.join(\"results\", \"eval\", \"robustness\")\n",
    "df_model_perf = None\n",
    "file_path = os.path.join(performance_dir, \"model_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_model_perf = pd.read_csv(file_path).copy()\n",
    "    df_model_perf = df_model_perf.set_index([\"benchmark\", \"model\"])\n",
    "file_path = os.path.join(performance_dir, \"detector_performance.csv\")\n",
    "if os.path.exists(file_path):\n",
    "    df_detector_perf = pd.read_csv(file_path).copy()\n",
    "    df_detector_perf = df_detector_perf.set_index([\"benchmark\", \"model\", \"dataset\", \"score_function\"])\n",
    "\n",
    "save_dir = os.path.join(\"results\", \"eval\", \"robustness\")\n",
    "assert os.path.exists(save_dir)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    \n",
    "    # ID dataset\n",
    "    # Load the MAE/DAE rate table on ID dataset.\n",
    "    df_id = None\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_id_local_mae_dae_rate_mean_std.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df_id = pd.read_csv(file_path).copy()\n",
    "        \n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].astype(\"category\")\n",
    "        df_id[\"perturb_function\"] = df_id[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].astype(\"category\")\n",
    "        df_id[\"variant\"] = df_id[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "\n",
    "        for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "            \n",
    "            # Read model accuracy for each model variant\n",
    "            accuracies = dict()\n",
    "            if df_model_perf is not None and (benchmark, model_name) in df_model_perf.index:\n",
    "                model_perf = df_model_perf.loc[[(benchmark, model_name)]].copy().round(3).reset_index(drop=True).set_index(\"variant\")\n",
    "                model_perf.index.name = None\n",
    "                accuracies = model_perf.to_dict()[\"accuracy\"]\n",
    "\n",
    "            # Read detector performance metrics for each model variant\n",
    "            detector_performances = dict()\n",
    "            for score_func in score_functions:\n",
    "                detector_perf = dict()\n",
    "                if (df_detector_perf is not None) and ((benchmark, model_name, \"average\", score_func) in df_detector_perf.index):\n",
    "                    detector_perf = df_detector_perf.loc[[(benchmark, model_name, \"average\", score_func)]].copy().round(3).reset_index(drop=True).set_index(\"variant\").T.to_dict()\n",
    "                detector_performances[score_func] = detector_perf\n",
    "            \n",
    "            # MAE rate of different model variants under various perturbations.\n",
    "            data = df_id[df_id[\"model\"]==model_name].copy()\n",
    "            data = data.sort_values(by=[\"perturb_function\", \"variant\"]).copy()\n",
    "            \n",
    "            draw_radar_plot(data=data, theta=\"perturb_function\", r=\"mae\", hue=\"variant\",\n",
    "                            title=f\"Average MAE rate (%) under different functional perturbations on ID dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_id_mae_perturb_function_radar_plot.png\"), refresh=refresh)\n",
    "            draw_line_chart(data=data, x=\"perturb_function\", y=\"mae\", hue=\"variant\", errorbar=True,\n",
    "                            title=f\"Average MAE rate (%) under different functional perturbations on ID dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_id_mae_perturb_function_line_chart.png\"), refresh=refresh)\n",
    "            \n",
    "            # ID DAE rate of different model variants under various perturbations.\n",
    "            dae_names = [col.replace(\"_mean\", \"\") for col in data.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            detector_names = [dae_name.replace(\"dae_\", \"\") for dae_name in dae_names]\n",
    "            subtitles = []\n",
    "            for d in detector_names:\n",
    "                if d in detector_performances:\n",
    "                    perf_str = \"\\n\".join([f\"{k}:{v}\" for k, v in detector_performances[d].items()])\n",
    "                    subtitles.append(f\"DAE rate - {d}\\n Detector performance:\\n {perf_str}\")\n",
    "                else:\n",
    "                    subtitles.append(f\"DAE rate - {d}\")\n",
    "            draw_radar_plot(data=data, theta=\"perturb_function\", r=dae_names, \n",
    "                            hue=\"variant\", title=f\"Average DAE rate (%) under different functional perturbations on ID dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_id_dae_perturb_function_radar_plot.png\"), \n",
    "                            subtitle=subtitles, refresh=refresh)\n",
    "            draw_line_chart(data=data, x=\"perturb_function\", y=dae_names, hue=\"variant\", errorbar=True,\n",
    "                            title=f\"Average DAE rate (%) under different functional perturbations on ID dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_id_dae_perturb_function_line_chart.png\"), refresh=refresh)\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "\n",
    "    # OOD dataset\n",
    "    # Load the DAE rate table on OoD dataset.\n",
    "    df_ood = None\n",
    "    file_path = os.path.join(robustness_dir, f\"{benchmark.lower()}_ood_local_dae_rate_mean_std.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df_ood = pd.read_csv(file_path).copy()\n",
    "    \n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].astype(\"category\")\n",
    "        df_ood[\"perturb_function\"] = df_ood[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].astype(\"category\")\n",
    "        df_ood[\"variant\"] = df_ood[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "\n",
    "        for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "\n",
    "            # Read model accuracy for each model variant\n",
    "            accuracies = dict()\n",
    "            if df_model_perf is not None and (benchmark, model_name) in df_model_perf.index:\n",
    "                model_perf = df_model_perf.loc[[(benchmark, model_name)]].copy().round(3).reset_index(drop=True).set_index(\"variant\")\n",
    "                model_perf.index.name = None\n",
    "                accuracies = model_perf.to_dict()[\"accuracy\"]\n",
    "\n",
    "            # Read detector performance metrics for each model variant\n",
    "            detector_performances = dict()\n",
    "            for score_func in score_functions:\n",
    "                detector_perf = dict()\n",
    "                if (df_detector_perf is not None) and ((benchmark, model_name, \"average\", score_func) in df_detector_perf.index):\n",
    "                    detector_perf = df_detector_perf.loc[[(benchmark, model_name, \"average\", score_func)]].copy().round(3).reset_index(drop=True).set_index(\"variant\").T.to_dict()\n",
    "                detector_performances[score_func] = detector_perf\n",
    "\n",
    "            # OoD DAE rate of different model variants under various perturbations.\n",
    "            data = df_ood[(df_ood[\"model\"]==model_name) & (df_ood[\"dataset\"]==\"average\")].copy()\n",
    "            data = data.sort_values(by=[\"perturb_function\", \"variant\"]).copy()\n",
    "\n",
    "            dae_names = [col.replace(\"_mean\", \"\") for col in data.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            detector_names = [dae_name.replace(\"dae_\", \"\") for dae_name in dae_names]\n",
    "            subtitles = []\n",
    "            for d in detector_names:\n",
    "                if d in detector_performances:\n",
    "                    perf_str = \"\\n\".join([f\"{k}:{v}\" for k, v in detector_performances[d].items()])\n",
    "                    subtitles.append(f\"DAE rate - {d}\\n Detector performance:\\n {perf_str}\")\n",
    "                else:\n",
    "                    subtitles.append(f\"DAE rate - {d}\")\n",
    "\n",
    "            draw_radar_plot(data=data, theta=\"perturb_function\", r=dae_names, \n",
    "                            hue=\"variant\", title=f\"Average DAE rate (%) under different functional perturbations on OoD dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_ood_dae_perturb_function_radar_plot.png\"), \n",
    "                            subtitle=subtitles, refresh=refresh)\n",
    "            draw_line_chart(data=data, x=\"perturb_function\", y=dae_names, hue=\"variant\", errorbar=True,\n",
    "                            title=f\"Average DAE rate (%) under different functional perturbations on OoD dataset.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_ood_dae_perturb_function_line_chart.png\"), refresh=refresh)\n",
    "            \n",
    "            # OoD DAE rate of different model variants on various OoD datasets.\n",
    "            data = df_ood[(df_ood[\"model\"]==model_name) & (df_ood[\"perturb_function\"]==\"average\")].copy()\n",
    "            data = data.sort_values(by=[\"variant\"]).copy()\n",
    "            \n",
    "            dae_names = [col.replace(\"_mean\", \"\") for col in data.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "            detector_names = [dae_name.replace(\"dae_\", \"\") for dae_name in dae_names]\n",
    "            subtitles = []\n",
    "            for d in detector_names:\n",
    "                if d in detector_performances:\n",
    "                    perf_str = \"\\n\".join([f\"{k}:{v}\" for k, v in detector_performances[d].items()])\n",
    "                    subtitles.append(f\"DAE rate - {d}\\n Detector performance:\\n {perf_str}\")\n",
    "                else:\n",
    "                    subtitles.append(f\"DAE rate - {d}\")\n",
    "\n",
    "            draw_radar_plot(data=data, theta=\"dataset\", r=dae_names,\n",
    "                            hue=\"variant\", title=f\"Average DAE rate (%) on different OoD datasets.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_ood_dae_dataset_radar_plot.png\"), \n",
    "                            subtitle=subtitles, refresh=refresh)\n",
    "            draw_line_chart(data=data, x=\"dataset\", y=dae_names, hue=\"variant\", errorbar=True,\n",
    "                            title=f\"Average DAE rate (%) on different OoD datasets.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies}.\",\n",
    "                            save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_ood_dae_dataset_line_chart.png\"), refresh=refresh)\n",
    "\n",
    "            # Demonstrate the details of DAE rate on each OoD dataset for each model variant.\n",
    "            variants = sorted(set(data[\"variant\"]), key=list(data[\"variant\"]).index)\n",
    "            for variant in variants:\n",
    "                data = df_ood[(df_ood[\"model\"]==model_name) & (df_ood[\"dataset\"]!=\"average\") & (df_ood[\"variant\"]==variant)].copy()\n",
    "                data = data.sort_values(by=[\"perturb_function\"]).copy()\n",
    "                dae_names = [col.replace(\"_mean\", \"\") for col in data.columns if (\"dae_\" in col) and (\"_mean\" in col)]\n",
    "                detector_names = [dae_name.replace(\"dae_\", \"\") for dae_name in dae_names]\n",
    "                subtitles = []\n",
    "                for d in detector_names:\n",
    "                    if (d in detector_performances) and( variant in detector_performances[d]):\n",
    "                        subtitles.append(f\"DAE rate - {d}\\n Detector performance:\\n {detector_performances[d][variant]}\")\n",
    "                    else:\n",
    "                        subtitles.append(f\"DAE rate - {d}\")\n",
    "\n",
    "                draw_radar_plot(data=data, theta=\"perturb_function\", r=dae_names, hue=\"dataset\",\n",
    "                                title=f\"Average DAE rate (%) under different functional perturbations on various OoD datasets.\\nBenchmark={benchmark}, model={model_name}, variant={variant}, model accuracy (%):{accuracies[variant] if variant in accuracies else 'N/A'}.\",\n",
    "                                save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_{variant}_ood_dae_details_radar_plot.png\"), \n",
    "                                subtitle=subtitles, refresh=refresh)\n",
    "                draw_line_chart(data=data, x=\"perturb_function\", y=dae_names, hue=\"dataset\", errorbar=True,\n",
    "                                title=f\"Average DAE rate (%) under different functional perturbations on various OoD datasets.\\nBenchmark={benchmark}, model={model_name}, variant={variant}, model accuracy (%):{accuracies[variant] if variant in accuracies else 'N/A'}.\",\n",
    "                                save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_{variant}_ood_dae_details_line_chart.png\"), refresh=refresh)\n",
    "    else:\n",
    "        print(\"File \"+file_path+\" does not exist!\")\n",
    "\n",
    "\n",
    "    # Compare the relationship among the three: MAE rate, ID DAE rate, and OoD DAE rate.\n",
    "    # Under different perturbations for each model variant.\n",
    "    if (df_id is not None) and (df_ood is not None):\n",
    "        df_id_ood = pd.merge(df_id, df_ood[df_ood[\"dataset\"]==\"average\"], \n",
    "                                on=[\"model\", \"variant\", \"perturb_function\"], \n",
    "                                how='inner', suffixes=[\"_id\", \"_ood\"])\n",
    "        # Assembly MAE/DAE table\n",
    "        df_all = None\n",
    "        for score_func in score_functions:\n",
    "            if (f\"dae_{score_func}_mean_id\" in df_id_ood.columns) and (f\"dae_{score_func}_mean_ood\" in df_id_ood.columns):\n",
    "                df = df_id_ood.melt(id_vars=[\"model\", \"variant\", \"perturb_function\"], \n",
    "                    value_vars=[\"mae_mean\", f\"dae_{score_func}_mean_id\", f\"dae_{score_func}_mean_ood\"],\n",
    "                    var_name=\"ae_type\", value_name=f\"ae_{score_func}_mean\")\n",
    "                df = df.replace(\"mae_mean\", \"mae\").replace(f\"dae_{score_func}_mean_id\", \"dae_id\").replace(f\"dae_{score_func}_mean_ood\", \"dae_ood\")\n",
    "                \n",
    "                if (f\"dae_{score_func}_std_id\" in df_id_ood.columns) and (f\"dae_{score_func}_std_ood\" in df_id_ood.columns):\n",
    "                    df_std = df_id_ood.melt(id_vars=[\"model\", \"variant\", \"perturb_function\"], \n",
    "                        value_vars=[\"mae_std\", f\"dae_{score_func}_std_id\", f\"dae_{score_func}_std_ood\"],\n",
    "                        var_name=\"ae_type\", value_name=f\"ae_{score_func}_std\")\n",
    "                    df_std = df_std.replace(\"mae_std\", \"mae\").replace(f\"dae_{score_func}_std_id\", \"dae_id\").replace(f\"dae_{score_func}_std_ood\", \"dae_ood\")\n",
    "                    df = pd.merge(df, df_std, on=[\"model\", \"variant\", \"perturb_function\", \"ae_type\"]).copy()\n",
    "                    \n",
    "                if df_all is None:\n",
    "                    df_all = df.copy()\n",
    "                else:\n",
    "                    df_all = pd.merge(df_all, df, on=[\"model\", \"variant\", \"perturb_function\", \"ae_type\"]).copy()\n",
    "        \n",
    "        if df_all is not None:\n",
    "            df_all[\"perturb_function\"] = df_all[\"perturb_function\"].astype(\"category\")\n",
    "            df_all[\"perturb_function\"] = df_all[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "            df_all[\"variant\"] = df_all[\"variant\"].astype(\"category\")\n",
    "            df_all[\"variant\"] = df_all[\"variant\"].cat.set_categories(variant_sorter, ordered=True)\n",
    "            variants = sorted(set(df_all[\"variant\"]), key=list(df_all[\"variant\"]).index)\n",
    "            \n",
    "            for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "\n",
    "                # Read model accuracy for each model variant\n",
    "                accuracies = dict()\n",
    "                if df_model_perf is not None and (benchmark, model_name) in df_model_perf.index:\n",
    "                    model_perf = df_model_perf.loc[[(benchmark, model_name)]].copy().round(3).reset_index(drop=True).set_index(\"variant\")\n",
    "                    model_perf.index.name = None\n",
    "                    accuracies = model_perf.to_dict()[\"accuracy\"]\n",
    "\n",
    "                # Read detector performance metrics for each model variant\n",
    "                detector_performances = dict()\n",
    "                for score_func in score_functions:\n",
    "                    detector_perf = dict()\n",
    "                    if (df_detector_perf is not None) and ((benchmark, model_name, \"average\", score_func) in df_detector_perf.index):\n",
    "                        detector_perf =df_detector_perf.loc[[(benchmark, model_name, \"average\", score_func)]].copy().round(3).reset_index(drop=True).set_index(\"variant\").T.to_dict()\n",
    "                    detector_performances[score_func] = detector_perf\n",
    "\n",
    "                for variant in variants:\n",
    "                    data = df_all[(df_all[\"model\"]==model_name) & (df_all[\"variant\"]==variant)].copy()\n",
    "                    data = data.sort_values(by=[\"perturb_function\", \"ae_type\"]).copy()\n",
    "                    # MAE, ID/OoD DAE rate under different perturbations for each model variant.\n",
    "                    dae_names = [col.replace(\"_mean\", \"\") for col in data.columns if (\"ae_\" in col) and (\"_mean\" in col)]\n",
    "                    detector_names = [dae_name.replace(\"ae_\", \"\") for dae_name in dae_names]\n",
    "                    subtitles = []\n",
    "                    for d in detector_names:\n",
    "                        if (d in detector_performances) and (variant in detector_performances[d]):\n",
    "                            subtitles.append(f\"{d}\\n Detector performance:\\n {detector_performances[d][variant]}\")\n",
    "                        else:\n",
    "                            subtitles.append(f\"{d}\")\n",
    "                    if len(dae_names) > 0:\n",
    "                        draw_radar_plot(data=data, theta=\"perturb_function\", r=dae_names,\n",
    "                                        hue=\"ae_type\", title=f\"Average MAE vs ID/OoD DAE rate (%) under different functional perturbations.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies[variant] if variant in accuracies else 'N/A'}.\",\n",
    "                                        save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_{variant}_id_ood_mae_dae_perturb_function_radar_plot.png\"), \n",
    "                                        subtitle=subtitles, refresh=refresh)\n",
    "                        draw_line_chart(data=data, x=\"perturb_function\", y=dae_names, hue=\"ae_type\", errorbar=True,\n",
    "                                        title=f\"Average MAE vs ID/OoD DAE rate (%) under different functional perturbations.\\nBenchmark={benchmark}, model={model_name}, model accuracy (%):{accuracies[variant] if variant in accuracies else 'N/A'}.\",\n",
    "                                        save_path=os.path.join(save_dir, \"figures\", f\"{benchmark.lower()}_{model_name}_{variant}_id_ood_mae_dae_perturb_function_line_chart.png\"), refresh=refresh)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

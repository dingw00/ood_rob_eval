{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook aims to study the perturbation-induced details, including OoD scores distribution shift and local error rate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /home/dingw/work/ood_robustness_eva\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "\n",
    "from utils.eval import get_ood_measures\n",
    "from utils.visualize import *\n",
    "\n",
    "# Load configs: benchmarks, model variants, OoD datasets and save directory.\n",
    "with open('config.yaml', 'r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "    \n",
    "score_functions = configs[\"score_functions\"]\n",
    "perturb_functions = configs[\"perturb_functions\"]\n",
    "rand_seed = configs[\"rand_seed\"]\n",
    "\n",
    "# Define the order of perturbation functions and model variants in visualizations.\n",
    "perturb_function_sorter = configs[\"perturb_functions\"]\n",
    "variant_sorter = [\"NT\", \"DA\", \"AT\", \"PAT\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. OoD scores distribution shift\n",
    "We show the distribution of ID & OoD datasets' OoD scores for both seeds and perturbed samples. A good post-hoc OoD detection method should have a good separation of OoD scores between ID & OoD datasets. \n",
    "\n",
    "We consider 2 benchmarks (CIFAR10, ImageNet100), 2 model architectures (WRN-40-2, ResNet50), and 4 model variants (NT, DA, AT, PAT) for each architecture. We show the OoD scores distrubution shift under 9 perturbation functions and for 12 OoD detectors.\n",
    "\n",
    "The visualizations are in the format of histogram and box plot. The generated figures are saved in `results/eval/performance/figures/statistics` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189758/4207973711.py:27: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axes = plt.subplots(nrows, ncols, figsize=(8*ncols, 4*nrows), layout=\"constrained\")\n"
     ]
    }
   ],
   "source": [
    "# OoD scores distribution for ID & OoD seeds and perturbed samples.\n",
    "save_dir = os.path.join(\"results\", \"eval\", \"performance\", \"figures\", \"statistics\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        for variant in configs[\"benchmark\"][benchmark][\"model\"][model_name]:\n",
    "            scores_dir = f\"results/{benchmark.lower()}/{rand_seed}/{model_name}/{variant}\"\n",
    "            \n",
    "            if not os.path.exists(os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\")):\n",
    "                print(\"File\", os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\"), \"does not exist.\")\n",
    "                continue\n",
    "            id_scores_seed = pd.read_csv(os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\")).copy()\n",
    "            id_scores_seed[\"dataset\"] = \"ID\"\n",
    "            for dataset in configs[\"benchmark\"][benchmark][\"ood_datasets\"]:\n",
    "                if not os.path.exists(os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\")):\n",
    "                    print(\"File\", os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\"), \"does not exist.\")\n",
    "                    continue\n",
    "                ood_scores_seed = pd.read_csv(os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\")).copy()\n",
    "                ood_scores_seed[\"dataset\"] = \"OoD\"\n",
    "\n",
    "                scores_seed = pd.concat([id_scores_seed, ood_scores_seed], axis=0).copy()\n",
    "\n",
    "                # Plot score distributions for seeds and perturbed samples under all perturbations.\n",
    "                ncols = len(score_functions)\n",
    "                nrows = 2\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(8*ncols, 4*nrows), layout=\"constrained\")\n",
    "                axes_seed = axes[0]\n",
    "                axes_perb = axes[1]\n",
    "                for ax_seed, ax_perb, score_func in zip(axes_seed, axes_perb, score_functions):\n",
    "                    scores_perb = pd.DataFrame()\n",
    "                    for perb_func in perturb_functions:\n",
    "\n",
    "                        id_scores_perb = pd.read_csv(os.path.join(scores_dir, benchmark, \"scores\", f\"perb_{perb_func}_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                        id_scores_perb[\"dataset\"] = \"ID\"\n",
    "\n",
    "                        ood_scores_perb = pd.read_csv(os.path.join(scores_dir, dataset, \"scores\", f\"perb_{perb_func}_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                        ood_scores_perb[\"dataset\"] = \"OoD\"\n",
    "\n",
    "                        scores_perb = pd.concat([scores_perb, id_scores_perb, ood_scores_perb], axis=0).copy()\n",
    "\n",
    "                    id_scores_seed_ = scores_seed[scores_seed[\"dataset\"]==\"ID\"][f\"{score_func}_score\"].to_numpy()\n",
    "                    ood_scores_seed_ = scores_seed[scores_seed[\"dataset\"]==\"OoD\"][f\"{score_func}_score\"].to_numpy()\n",
    "                    id_scores_perb_ = scores_perb[scores_perb[\"dataset\"]==\"ID\"][f\"{score_func}_score\"].to_numpy()\n",
    "                    ood_scores_perb_ = scores_perb[scores_perb[\"dataset\"]==\"OoD\"][f\"{score_func}_score\"].to_numpy()\n",
    "                    auroc_seed, aupr_in_seed, aupr_out_seed, fpr_seed, threshold_seed, _, _ = \\\n",
    "                        get_ood_measures(id_scores_seed_, ood_scores_seed_)\n",
    "                    auroc_perb, aupr_in_perb, aupr_out_perb, fpr_perb, threshold_perb, _, _ = \\\n",
    "                        get_ood_measures(id_scores_seed_, ood_scores_perb_)\n",
    "                \n",
    "                    sns.histplot(data=scores_seed, x=f\"{score_func}_score\", hue=\"dataset\", stat=\"percent\", \n",
    "                                    bins=30, common_norm=False, ax=ax_seed)\n",
    "                    sns.histplot(data=scores_perb, x=f\"{score_func}_score\", hue=\"dataset\", stat=\"percent\",\n",
    "                                    bins=30, common_norm=False, ax=ax_perb)\n",
    "                    ax_seed.axvline(x=-threshold_seed, color=\"red\", linestyle=\"--\")\n",
    "                    ax_perb.axvline(x=-threshold_perb, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "                    min_ = min(scores_seed[f\"{score_func}_score\"].min(), scores_perb[f\"{score_func}_score\"].min())\n",
    "                    max_ = max(scores_seed[f\"{score_func}_score\"].max(), scores_perb[f\"{score_func}_score\"].max())\n",
    "                    ax_seed.set_xlim([min_, max_])\n",
    "                    ax_perb.set_xlim([min_, max_])\n",
    "                    _, ymax0 = ax_seed.set_ylim()\n",
    "                    _, ymax1 = ax_perb.set_ylim()\n",
    "                    ymax = max(ymax0, ymax1)\n",
    "                    ax_seed.set_ylim([0, ymax])\n",
    "                    ax_perb.set_ylim([0, ymax])\n",
    "        \n",
    "                    ax_seed.set_xlabel(f\"{score_func} score\")\n",
    "                    ax_perb.set_xlabel(f\"{score_func} score\")\n",
    "                    ax_seed.set_title(f\"{score_func} score distribution\\nSeeds: FPR95={fpr_seed*100:.2f}%, AUROC={auroc_seed:.2f}, AUPR={aupr_in_seed:.2f}\", loc=\"left\")\n",
    "                    ax_perb.set_title(f\"Perturbed samples: FPR95={fpr_perb*100:.2f}%, AUROC={auroc_perb:.2f}, AUPR={aupr_in_perb:.2f}\", loc=\"left\")\n",
    "\n",
    "                plt.suptitle(f\"OoD scores distribution for seeds and perturbed samples\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}, OoD dataset={dataset}, perturbation=all\")\n",
    "                plt.savefig(os.path.join(save_dir, f\"ood_scores_{benchmark}_{model_name}_{variant}_{dataset}_hist.png\"))\n",
    "                \n",
    "                # Plot score distributions for seeds and perturbed samples under each perturbation.\n",
    "                for perb_func in perturb_functions:\n",
    "\n",
    "                    id_scores_perb = pd.read_csv(os.path.join(scores_dir, benchmark, \"scores\", f\"perb_{perb_func}_scores.csv\")).copy().drop([\"idx\",\"y_true\", \"y_pred\", \"idx_suffix\"], axis=1)\n",
    "                    id_scores_perb[\"dataset\"] = \"ID\"\n",
    "\n",
    "                    ood_scores_perb = pd.read_csv(os.path.join(scores_dir, dataset, \"scores\", f\"perb_{perb_func}_scores.csv\")).copy().drop([\"idx\",\"y_true\", \"y_pred\", \"idx_suffix\"], axis=1)\n",
    "                    ood_scores_perb[\"dataset\"] = \"OoD\"\n",
    "\n",
    "                    scores_perb = pd.concat([id_scores_perb, ood_scores_perb], axis=0).copy()\n",
    "\n",
    "                    ncols = len(score_functions)\n",
    "                    nrows = 2\n",
    "                    fig, axes = plt.subplots(nrows, ncols, figsize=(8*ncols, 4*nrows), layout=\"constrained\")\n",
    "                    axes_seed = axes[0]\n",
    "                    axes_perb = axes[1]\n",
    "                    for ax_seed, ax_perb, score_func in zip(axes_seed, axes_perb, score_functions):\n",
    "                        \n",
    "                        id_scores_seed_ = scores_seed[scores_seed[\"dataset\"]==\"ID\"][f\"{score_func}_score\"].to_numpy()\n",
    "                        ood_scores_seed_ = scores_seed[scores_seed[\"dataset\"]==\"OoD\"][f\"{score_func}_score\"].to_numpy()\n",
    "                        id_scores_perb_ = scores_perb[scores_perb[\"dataset\"]==\"ID\"][f\"{score_func}_score\"].to_numpy()\n",
    "                        ood_scores_perb_ = scores_perb[scores_perb[\"dataset\"]==\"OoD\"][f\"{score_func}_score\"].to_numpy()\n",
    "                        \n",
    "                        auroc_seed, aupr_in_seed, aupr_out_seed, fpr_seed, threshold_seed, _, _ = \\\n",
    "                            get_ood_measures(id_scores_seed_, ood_scores_seed_)\n",
    "                        auroc_perb, aupr_in_perb, aupr_out_perb, fpr_perb, threshold_perb, _, _ = \\\n",
    "                            get_ood_measures(id_scores_seed_, ood_scores_perb_)\n",
    "\n",
    "                        sns.histplot(data=scores_seed, x=f\"{score_func}_score\", hue=\"dataset\", stat=\"percent\", \n",
    "                                        bins=30, common_norm=False, ax=ax_seed)\n",
    "                        sns.histplot(data=scores_perb, x=f\"{score_func}_score\", hue=\"dataset\", stat=\"percent\",\n",
    "                                        bins=30, common_norm=False, ax=ax_perb)\n",
    "                        \n",
    "                        ax_seed.axvline(x=-threshold_seed, color=\"red\", linestyle=\"--\")\n",
    "                        ax_perb.axvline(x=-threshold_perb, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "                        min_ = min(scores_seed[f\"{score_func}_score\"].min(), scores_perb[f\"{score_func}_score\"].min())\n",
    "                        max_ = max(scores_seed[f\"{score_func}_score\"].max(), scores_perb[f\"{score_func}_score\"].max())\n",
    "                        ax_seed.set_xlim(min_, max_)\n",
    "                        ax_perb.set_xlim(min_, max_)\n",
    "                        _, ymax0 = ax_seed.set_ylim()\n",
    "                        _, ymax1 = ax_perb.set_ylim()\n",
    "                        ymax = max(ymax0, ymax1)\n",
    "                        ax_seed.set_ylim(0, ymax)\n",
    "                        ax_perb.set_ylim(0, ymax)\n",
    "        \n",
    "                        ax_seed.set_xlabel(f\"{score_func} score\")\n",
    "                        ax_perb.set_xlabel(f\"{score_func} score\")\n",
    "\n",
    "                        ax_seed.set_title(f\"{score_func} score distribution\\nSeeds: FPR95={fpr_seed*100:.2f}%, AUROC={auroc_seed:.2f}, AUPR={aupr_in_seed:.2f}\", loc=\"left\")\n",
    "                        ax_perb.set_title(f\"Perturbed samples: FPR95={fpr_perb*100:.2f}%, AUROC={auroc_perb:.2f}, AUPR={aupr_in_perb:.2f}\", loc=\"left\")\n",
    "\n",
    "                    plt.suptitle(f\"OoD scores distribution for seeds and perturbed samples\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}, OoD dataset={dataset}, perturbation={perb_func}\")\n",
    "                    plt.savefig(os.path.join(save_dir, f\"ood_scores_{benchmark}_{model_name}_{variant}_{dataset}_{perb_func}_hist.png\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of OoD scores for ID & OoD seeds and perturbed samples\n",
    "save_dir = os.path.join(\"results\", \"eval\", \"performance\", \"figures\", \"statistics\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    dataset_sorter = [benchmark] + configs[\"benchmark\"][benchmark][\"ood_datasets\"]\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        for variant in configs[\"benchmark\"][benchmark][\"model\"][model_name]:\n",
    "            scores_dir = f\"results/{benchmark.lower()}/{rand_seed}/{model_name}/{variant}\"\n",
    "            \n",
    "            if not os.path.exists(os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\")):\n",
    "                print(\"File\", os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\"), \"does not exist.\")\n",
    "                continue\n",
    "            \n",
    "            for dataset in configs[\"benchmark\"][benchmark][\"ood_datasets\"]:\n",
    "                if not os.path.exists(os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\")):\n",
    "                    print(\"File\", os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\"), \"does not exist.\")\n",
    "                    continue\n",
    "                \n",
    "                # Box plot of OoD scores for seeds and perturbed samples under different perturbations.\n",
    "                ncols = 2\n",
    "                nrows = len(score_functions)\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(10*ncols, 4*nrows), layout=\"constrained\")\n",
    "                axes = axes.flatten()\n",
    "\n",
    "                for i, score_func in enumerate(score_functions):\n",
    "\n",
    "                    ax_id = axes[2*i]\n",
    "                    ax_ood = axes[2*i+1]\n",
    "\n",
    "                    id_scores_seed = pd.read_csv(os.path.join(scores_dir, benchmark, \"scores\",\"temp_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                    id_scores_seed[\"distribution\"] = \"ID\"\n",
    "                    id_scores_seed[\"dataset\"] = benchmark\n",
    "                    id_scores_seed[\"perturb_function\"] = \"original\"\n",
    "\n",
    "                    ood_scores_seed = pd.read_csv(os.path.join(scores_dir, dataset, \"scores\",\"temp_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                    ood_scores_seed[\"distribution\"] = \"OoD\"\n",
    "                    ood_scores_seed[\"dataset\"] = dataset\n",
    "                    ood_scores_seed[\"perturb_function\"] = \"original\"\n",
    "\n",
    "                    auroc_seed, aupr_in_seed, aupr_out_seed, fpr_seed, threshold_seed, _, _ = \\\n",
    "                        get_ood_measures(id_scores_seed[f\"{score_func}_score\"].to_numpy(), ood_scores_seed[f\"{score_func}_score\"].to_numpy())\n",
    "\n",
    "                    id_scores = id_scores_seed.copy()\n",
    "                    for perb_func in perturb_functions:\n",
    "\n",
    "                        id_scores_perb = pd.read_csv(os.path.join(scores_dir, benchmark, \"scores\", f\"perb_{perb_func}_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                        id_scores_perb[\"dataset\"] = benchmark\n",
    "                        id_scores_perb[\"perturb_function\"] = perb_func\n",
    "\n",
    "                        id_scores = pd.concat([id_scores, id_scores_perb], axis=0).copy()\n",
    "                    \n",
    "                    sns.boxplot(data=id_scores, x=\"perturb_function\", y=f\"{score_func}_score\",\n",
    "                                ax=ax_id, width=.1, fill=False, fliersize=5, color='k')\n",
    "                    sns.pointplot(data=id_scores, x=\"perturb_function\", y=f\"{score_func}_score\",\n",
    "                                  errorbar=\"sd\", capsize=.2, color=\"r\", linewidth=2, \n",
    "                                  linestyle=\"none\", markers=\"_\", markersize=40, ax=ax_id)\n",
    "                \n",
    "                    del id_scores\n",
    "                    \n",
    "                    ax_id.axhline(-threshold_seed, color=\"red\", linestyle=\"--\")\n",
    "                    ax_id.set_xlabel(f\"Perturbation\")\n",
    "                    ax_id.set_title(f\"Detector={score_func}, Dataset=ID\", loc=\"left\")\n",
    "                    ymin0, ymax0 = ax_id.set_ylim()\n",
    "\n",
    "                    ood_scores = ood_scores_seed.copy()\n",
    "                    for perb_func in perturb_functions:\n",
    "\n",
    "                        ood_scores_perb = pd.read_csv(os.path.join(scores_dir, dataset, \"scores\", f\"perb_{perb_func}_scores.csv\"))[[f\"{score_func}_score\"]].copy()\n",
    "                        ood_scores_perb[\"dataset\"] = dataset\n",
    "                        ood_scores_perb[\"perturb_function\"] = perb_func\n",
    "\n",
    "                        ood_scores = pd.concat([ood_scores, ood_scores_perb], axis=0).copy()\n",
    "\n",
    "                    sns.boxplot(data=ood_scores, x=\"perturb_function\", y=f\"{score_func}_score\",\n",
    "                                ax=ax_ood, width=.1, fill=False, fliersize=5, color='k')\n",
    "                    sns.pointplot(data=ood_scores, x=\"perturb_function\", y=f\"{score_func}_score\",\n",
    "                                  errorbar=\"sd\", capsize=.2, color=\"b\", linewidth=2, \n",
    "                                  linestyle=\"none\", markers=\"_\", markersize=40, ax=ax_ood,\n",
    "                                  )\n",
    "                    del ood_scores\n",
    "\n",
    "                    ax_ood.axhline(-threshold_seed, color=\"red\", linestyle=\"--\")\n",
    "                    ax_ood.set_xlabel(f\"Perturbation\")\n",
    "                    ax_ood.set_title(f\"Dataset=OoD\", loc=\"left\")\n",
    "                    ymin1, ymax1 = ax_ood.set_ylim()\n",
    "\n",
    "                    ymin = min(ymin0, ymin1)\n",
    "                    ymax = max(ymax0, ymax1)\n",
    "\n",
    "                    ax_id.set_ylim(ymin, ymax)\n",
    "                    ax_ood.set_ylim(ymin, ymax)\n",
    "\n",
    "\n",
    "                plt.suptitle(f\"OoD scores distribution for seeds and perturbed samples\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}, OoD dataset={dataset}\")\n",
    "                plt.savefig(os.path.join(save_dir, f\"ood_scores_{benchmark}_{model_name}_{variant}_{dataset}_box.png\"))\n",
    "                plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Local DAE rate statistics\n",
    "We show the distribution of local DAE rate under various functional perturbations and for different OoD datasets in the format of histogram. The average DAE rate with 95% confidence interval under each perturbation for each OoD dataset are shown in the format of bar plot.\n",
    "\n",
    "The generated figures are saved in `results/eval/robustness/figures/statistics/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of local DAE rate for ID & OoD seeds.\n",
    "dae_record_dir = os.path.join(\"results\", \"eval\", \"intermediate_results\")\n",
    "save_dir = os.path.join(\"results\", \"eval\", \"robustness\", \"figures\", \"statistics\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    dataset_sorter = [benchmark] + configs[\"benchmark\"][benchmark][\"ood_datasets\"]\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        for variant in configs[\"benchmark\"][benchmark][\"model\"][model_name]:\n",
    "            \n",
    "            # Histogram of local DAE rate for ID seeds under different perturbations.\n",
    "            filepath = os.path.join(dae_record_dir, f\"{benchmark.lower()}_{model_name}_{variant}\", \n",
    "                                               \"id_local_mae_dae_rate.csv\")\n",
    "            if not os.path.exists(filepath):\n",
    "                print(\"File\", filepath, \"does not exist.\")\n",
    "            else:\n",
    "                df_id_dae = pd.read_csv(filepath).copy()\n",
    "                df_id_dae[\"perturb_function\"] = df_id_dae[\"perturb_function\"].astype(\"category\")\n",
    "                df_id_dae[\"perturb_function\"] = df_id_dae[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "\n",
    "                ncols = 1\n",
    "                nrows = len(score_functions)\n",
    "                fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows), layout=\"constrained\")\n",
    "                axes = axes.flatten()\n",
    "                for ax, score_func in zip(axes, score_functions):\n",
    "                    if f\"dae_{score_func}\" not in df_id_dae.columns:\n",
    "                        continue\n",
    "                    \n",
    "                    df_id_dae[f\"dae_{score_func}\"] = df_id_dae[f\"dae_{score_func}\"] * 100\n",
    "                    \n",
    "                    sns.histplot(data=df_id_dae, x=f\"dae_{score_func}\", hue=\"perturb_function\", \n",
    "                                 stat=\"percent\", ax=ax, legend=True, shrink=0.8,\n",
    "                                 bins=30, common_norm=False, multiple=\"dodge\", palette=\"tab10\")\n",
    "                    \n",
    "                    ax.set_xlabel(\"DAE rate (%)\")\n",
    "                    ax.set_ylabel(\"Number of seeds\")\n",
    "                    ax.set_title(f\"Detector={score_func}\")\n",
    "                    ax.set_xlim(0, 100)\n",
    "\n",
    "                plt.suptitle(f\"Local DAE rate distribution for ID seeds.\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}\")\n",
    "                plt.savefig(os.path.join(save_dir, f\"{benchmark.lower()}_{model_name}_{variant}_id_local_dae_rate_perturb_hist.png\"))\n",
    "            \n",
    "            # Histogram of local DAE rate for OoD seeds under different perturbations.\n",
    "            df_ood_dae = pd.DataFrame()\n",
    "            for dataset in configs[\"benchmark\"][benchmark][\"ood_datasets\"]:\n",
    "                filepath = os.path.join(dae_record_dir, f\"{benchmark.lower()}_{model_name}_{variant}\", \n",
    "                                               f\"ood_local_dae_rate_{dataset}.csv\")\n",
    "                if not os.path.exists(filepath):\n",
    "                    print(\"File\", filepath, \"does not exist.\")\n",
    "                else:\n",
    "                    df = pd.read_csv(filepath).copy()\n",
    "                    df[\"dataset\"] = dataset\n",
    "                    df_ood_dae = pd.concat([df_ood_dae, df], ignore_index=True)\n",
    "\n",
    "            df_ood_dae[\"perturb_function\"] = df_ood_dae[\"perturb_function\"].astype(\"category\")\n",
    "            df_ood_dae[\"perturb_function\"] = df_ood_dae[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "\n",
    "            ncols = 1\n",
    "            nrows = len(score_functions)\n",
    "            fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows), layout=\"constrained\" )\n",
    "            axes = axes.flatten()\n",
    "            for ax, score_func in zip(axes, score_functions):\n",
    "                if f\"dae_{score_func}\" not in df_ood_dae.columns:\n",
    "                    continue\n",
    "                \n",
    "                df_ood_dae[f\"dae_{score_func}\"] = df_ood_dae[f\"dae_{score_func}\"] * 100\n",
    "                \n",
    "                sns.histplot(data=df_ood_dae, x=f\"dae_{score_func}\", hue=\"perturb_function\", \n",
    "                            stat=\"percent\", ax=ax, legend=True, shrink=0.8,\n",
    "                            bins=30, common_norm=False, multiple=\"dodge\", palette=\"tab10\")\n",
    "                \n",
    "                ax.set_xlabel(\"DAE rate (%)\")\n",
    "                ax.set_ylabel(\"Number of seeds\")\n",
    "                ax.set_title(f\"Detector={score_func}\")\n",
    "                ax.set_xlim(0, 100)\n",
    "\n",
    "            plt.suptitle(f\"Local DAE rate distribution for OoD seeds.\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}\",)\n",
    "            plt.savefig(os.path.join(save_dir, f\"{benchmark.lower()}_{model_name}_{variant}_ood_local_dae_rate_perturb_hist.png\"))\n",
    "\n",
    "            # Histogram of local DAE rate for OoD seeds from different datasets.\n",
    "            ncols = 1\n",
    "            nrows = len(score_functions)\n",
    "            fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows), layout=\"constrained\")\n",
    "            axes = axes.flatten()\n",
    "            for ax, score_func in zip(axes, score_functions):\n",
    "                if f\"dae_{score_func}\" not in df_ood_dae.columns:\n",
    "                    continue\n",
    "                                \n",
    "                sns.histplot(data=df_ood_dae, x=f\"dae_{score_func}\", hue=\"dataset\", \n",
    "                            stat=\"percent\", ax=ax, legend=True, shrink=0.8,\n",
    "                            bins=30, common_norm=False, multiple=\"dodge\", palette=\"tab10\")\n",
    "                \n",
    "                ax.set_xlabel(\"DAE rate (%)\")\n",
    "                ax.set_ylabel(\"Number of seeds\")\n",
    "                ax.set_title(f\"Detector={score_func}\")\n",
    "                ax.set_xlim(0, 100)\n",
    "\n",
    "            plt.suptitle(f\"Local DAE rate distribution for OoD seeds.\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}\",\n",
    "                        y=1.02)\n",
    "            plt.savefig(os.path.join(save_dir, f\"{benchmark.lower()}_{model_name}_{variant}_ood_local_dae_rate_dataset_hist.png\"))\n",
    "            plt.close(\"all\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of local MAE rate for ID seeds.\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    dataset_sorter = [benchmark] + configs[\"benchmark\"][benchmark][\"ood_datasets\"]\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        for variant in configs[\"benchmark\"][benchmark][\"model\"][model_name]:\n",
    "            \n",
    "            # Histogram of local DAE rate for ID seeds under different perturbations.\n",
    "            filepath = os.path.join(dae_record_dir, f\"{benchmark.lower()}_{model_name}_{variant}\", \n",
    "                                               \"id_local_mae_dae_rate.csv\")\n",
    "            if not os.path.exists(filepath):\n",
    "                print(\"File\", filepath, \"does not exist.\")\n",
    "            else:\n",
    "                df_id_dae = pd.read_csv(filepath).copy()\n",
    "                df_id_dae[\"perturb_function\"] = df_id_dae[\"perturb_function\"].astype(\"category\")\n",
    "                df_id_dae[\"perturb_function\"] = df_id_dae[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "\n",
    "                ncols = 1\n",
    "                nrows = 1\n",
    "                fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows), layout=\"constrained\")\n",
    "                if f\"mae\" not in df_id_dae.columns:\n",
    "                    continue\n",
    "                \n",
    "                df_id_dae[f\"mae\"] = df_id_dae[f\"mae\"] * 100\n",
    "                \n",
    "                sns.histplot(data=df_id_dae, x=f\"mae\", hue=\"perturb_function\", \n",
    "                                stat=\"percent\", ax=ax, legend=True, shrink=0.8,\n",
    "                                bins=30, common_norm=False, multiple=\"dodge\", palette=\"tab10\")\n",
    "                \n",
    "                ax.set_xlabel(\"MAE rate (%)\")\n",
    "                ax.set_ylabel(\"Number of seeds\")\n",
    "                ax.set_xlim(0, 100)\n",
    "                ax.set_title(f\"Local MAE rate distribution for ID seeds.\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}\")\n",
    "            \n",
    "            plt.savefig(os.path.join(save_dir, f\"{benchmark.lower()}_{model_name}_{variant}_id_local_mae_rate_perturb_hist.png\"))\n",
    "            plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average DAE rate bar plot for different OoD datasets & perturbation functions.\n",
    "dae_record_dir = os.path.join(\"results\", \"eval\", \"intermediate_results\")\n",
    "save_dir = os.path.join(\"results\", \"eval\", \"robustness\", \"figures\", \"statistics\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    dataset_sorter = [benchmark] + configs[\"benchmark\"][benchmark][\"ood_datasets\"]\n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        for variant in configs[\"benchmark\"][benchmark][\"model\"][model_name]:\n",
    "            \n",
    "            df_ood_dae = pd.DataFrame()\n",
    "            for dataset in configs[\"benchmark\"][benchmark][\"ood_datasets\"]:\n",
    "                filepath = os.path.join(dae_record_dir, f\"{benchmark.lower()}_{model_name}_{variant}\", \n",
    "                                               f\"ood_local_dae_rate_{dataset}.csv\")\n",
    "                if not os.path.exists(filepath):\n",
    "                    print(\"File\", filepath, \"does not exist.\")\n",
    "                else:\n",
    "                    df = pd.read_csv(filepath).copy()\n",
    "                    df[\"dataset\"] = dataset\n",
    "                    df_ood_dae = pd.concat([df_ood_dae, df], ignore_index=True)\n",
    "\n",
    "            df_ood_dae[\"perturb_function\"] = df_ood_dae[\"perturb_function\"].astype(\"category\")\n",
    "            df_ood_dae[\"perturb_function\"] = df_ood_dae[\"perturb_function\"].cat.set_categories(perturb_function_sorter, ordered=True)\n",
    "\n",
    "            ncols = 1\n",
    "            nrows = len(score_functions)\n",
    "            fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows), layout=\"constrained\")\n",
    "            axes = axes.flatten()\n",
    "            for ax, score_func in zip(axes, score_functions):\n",
    "                if f\"dae_{score_func}\" not in df_ood_dae.columns:\n",
    "                    continue\n",
    "                \n",
    "                df_ood_dae[f\"dae_{score_func}\"] = df_ood_dae[f\"dae_{score_func}\"] * 100\n",
    "                \n",
    "                sns.barplot(data=df_ood_dae, y=f\"dae_{score_func}\", x=\"perturb_function\",\n",
    "                             hue=\"dataset\", ax=ax, # errorbar=\"sd\", \n",
    "                             )\n",
    "                \n",
    "                ax.set_xlabel(\"DAE rate (%)\")\n",
    "                ax.set_ylabel(\"Number of seeds\")\n",
    "                ax.set_title(f\"Detector={score_func}\")\n",
    "                \n",
    "            plt.suptitle(f\"Average DAE rate for OoD seeds from different datasets.\\nBenchmark={benchmark}, Model={model_name}, Variant={variant}\")\n",
    "            plt.savefig(os.path.join(save_dir, f\"{benchmark.lower()}_{model_name}_{variant}_ood_dae_rate_perturb_dataset_bar.png\"))\n",
    "            plt.close(\"all\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

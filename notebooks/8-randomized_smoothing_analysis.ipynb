{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook aims to evaluate the effectiveness of randomized smoothing methods applied on OoD detectors.\n",
    "\n",
    "The raw experiment results should be stored in the `results/benchmark/rand_seed/model/variant/dataset/rs_scores/` folder aforehand. To regenerate the OoD scores, please run `randomized_smoothing_test.py`.\n",
    "\n",
    "Specifically, this notebook analyses the raw experiment results and provides a summary of the OoD detectors' performance (FPR95) and robustness (average DAE rate) metrics applying 3 methods:\n",
    "- Normal test\n",
    "- Randomized smoothing - Majority voting \n",
    "- Randomized smoothing - Averaging\n",
    "\n",
    "The result files are saved in the `results/eval/randomized_smoothing/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: wrn_40_2 Variant: NT\n",
      "Dataset: CIFAR10\n",
      "Dataset: Textures\n",
      "Dataset: SVHN\n",
      "Dataset: LSUN-C\n",
      "Dataset: LSUN-R\n",
      "Dataset: iSUN\n",
      "Dataset: Places365\n",
      "Results saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "from utils.eval import get_thr_tpr\n",
    "\n",
    "def df_sorted(df, sorter_dict):\n",
    "    for col, sorter in sorter_dict.items():\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        df[col] = df[col].cat.set_categories(sorter, ordered=True)\n",
    "    df = df.sort_values(list(sorter_dict.keys())).copy()\n",
    "    return df\n",
    "\n",
    "# Load configs: benchmarks, model variants, OoD datasets and save directory.\n",
    "with open('config.yaml', 'r') as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n",
    "score_functions = configs[\"score_functions\"]\n",
    "perturb_functions = configs[\"perturb_functions\"]\n",
    "rand_seed = configs[\"rand_seed\"]\n",
    "batch_size = configs[\"batch_size\"]\n",
    "n_seeds = configs[\"n_seeds\"]\n",
    "n_sampling = configs[\"n_sampling\"]\n",
    "n_rs = configs[\"n_randomized_smoothing\"]\n",
    "\n",
    "method_sorter = [\"seed\", \"perb\", \"voting\", \"avg\"]\n",
    "\n",
    "for benchmark in configs[\"benchmark\"]:\n",
    "    print(\"========================================\")\n",
    "    print(\"Benchmark:\", benchmark)\n",
    "    \n",
    "    ood_datasets = configs[\"benchmark\"][benchmark][\"ood_datasets\"]\n",
    "    dataset_sorter = [\"average\", benchmark] + ood_datasets\n",
    "    \n",
    "    for model_name in configs[\"benchmark\"][benchmark][\"model\"]:\n",
    "        print(\"========================================\")\n",
    "        print(\"Model:\", model_name)\n",
    "        for variant, weight_name in configs[\"benchmark\"][benchmark][\"model\"][model_name].items():\n",
    "            \n",
    "            save_dir = os.path.join(\"results\", \"eval\", \"randomized_smoothing\")\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print(\"----------------------------------------\")\n",
    "            print(\"Variant:\", variant)\n",
    "            df_fpr95_all = pd.DataFrame()\n",
    "            df_dae_all = pd.DataFrame()\n",
    "            df_acc = pd.DataFrame()\n",
    "            df_mae = pd.DataFrame()\n",
    "\n",
    "            rlt_dir = os.path.join(\"results\", benchmark.lower(), str(rand_seed), model_name, variant, benchmark, \"scores\")\n",
    "            filepath_all = os.path.join(rlt_dir, \"temp_all_scores.csv\")\n",
    "            if(not os.path.exists(filepath_all)):\n",
    "                continue\n",
    "            thr_dict = get_thr_tpr(filepath_all, score_functions=score_functions, tpr=0.95)\n",
    "            \n",
    "            for dataset in [benchmark]+ood_datasets:\n",
    "                print(\"Dataset:\", dataset)\n",
    "                rlt_dir = os.path.join(\"results\", benchmark.lower(), str(rand_seed), model_name, variant, dataset, \"scores\")\n",
    "                rs_rlt_dir = os.path.join(\"results\", benchmark.lower(), str(rand_seed), model_name, variant, dataset, \"rs_scores\")\n",
    "                filepath = os.path.join(rlt_dir, \"temp_scores.csv\")\n",
    "                if (not os.path.exists(filepath)):\n",
    "                    continue\n",
    "                df_temp = pd.read_csv(filepath).copy()\n",
    "                df_temp_ood = df_temp[[\"idx\", \"y_true\", \"y_pred\"]].copy()\n",
    "                for score_func in score_functions:\n",
    "                    if (f\"{score_func}_score\" in df_temp.columns) and (f\"{score_func}_score\" in thr_dict):\n",
    "                        df_temp_ood[f\"{score_func}_ood\"] = df_temp[f\"{score_func}_score\"] > thr_dict[f\"{score_func}_score\"]\n",
    "\n",
    "                df_perb_rs = pd.DataFrame()\n",
    "                for perb_func in perturb_functions:\n",
    "                    filepath_rs = os.path.join(rs_rlt_dir, f\"perb_{perb_func}_scores.csv\")\n",
    "                    if (not os.path.exists(filepath_rs)):\n",
    "                        continue\n",
    "                    df_perb = pd.read_csv(filepath_rs).copy()\n",
    "                    cols = [col for col in df_perb.columns if \"_score\" in col]\n",
    "                    df_perb = df_perb.drop(cols, axis=1)\n",
    "                    df_perb[\"perturb_function\"] = perb_func\n",
    "                    df_perb_rs = pd.concat([df_perb_rs, df_perb], axis=0, ignore_index=True).copy()\n",
    "                \n",
    "                if len(df_perb_rs) == 0:\n",
    "                    continue\n",
    "                df_ood = pd.merge(df_perb_rs, df_temp_ood, on=\"idx\", suffixes=[\"_perb\", \"_seed\"])\n",
    "                \n",
    "                # Calculate FPR95 (only for OoD datasets)\n",
    "                if dataset != benchmark:\n",
    "                    cols = [col for col in df_ood.columns if \"_ood\" in col]\n",
    "                    df_fpr95 = df_ood[cols].apply(lambda x: ~x).mean() * 100\n",
    "                    df_fpr95.name = \"fpr95\"\n",
    "                    df_fpr95 = df_fpr95.to_frame()\n",
    "                    df_fpr95.index.name = \"detector_method\"\n",
    "\n",
    "                    df_fpr95.reset_index(inplace=True)\n",
    "                    df_fpr95[\"method\"] = df_fpr95[\"detector_method\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "                    df_fpr95[\"detector\"] = df_fpr95[\"detector_method\"].apply(lambda x: x.split(\"_\")[0])\n",
    "                    df_fpr95.drop(\"detector_method\", axis=1, inplace=True)\n",
    "                    df_fpr95[\"dataset\"] = dataset\n",
    "                    df_fpr95_all = pd.concat([df_fpr95_all, df_fpr95], axis=0, ignore_index=True).copy()\n",
    "                \n",
    "                # Calculate model accuracy (only for ID dataset)\n",
    "                if dataset == benchmark:\n",
    "                    cols = [col for col in df_ood.columns if \"y_pred\" in col]\n",
    "                    df_ood[cols] = df_ood[cols].apply(lambda x: x==df_ood[\"y_true\"])\n",
    "                    df_acc = df_ood[cols].mean() * 100\n",
    "                    df_acc.name = \"accuracy\"\n",
    "                    df_acc = df_acc.to_frame()\n",
    "                    df_acc.index.name = \"method\"\n",
    "                    df_acc.reset_index(inplace=True)\n",
    "                    df_acc[\"method\"] = df_acc[\"method\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "\n",
    "                # Calculate average DAE rate\n",
    "                df_dae = pd.DataFrame()\n",
    "                for score_func in score_functions:\n",
    "                    if (f\"{score_func}_ood_seed\" in df_ood.columns):\n",
    "                        # select correctly detected seeds\n",
    "                        df_ood_ = df_ood[df_ood[f\"{score_func}_ood_seed\"]==(dataset!=benchmark)].copy()\n",
    "                        df_ood_ = df_ood_[[f\"{score_func}_ood_perb\", \n",
    "                                          f\"{score_func}_ood_voting\", f\"{score_func}_ood_avg\"]].copy()\n",
    "                        \n",
    "                        df_dae_ = (df_ood_==(dataset==benchmark)).copy()\n",
    "\n",
    "                        dae_mean = df_dae_.mean() * 100\n",
    "                        dae_std = df_dae_.std() * 100\n",
    "                        df_dae_ = pd.DataFrame({\"dae_mean\": dae_mean, \"dae_std\": dae_std})\n",
    "                        df_dae = pd.concat([df_dae, df_dae_], axis=0).copy()\n",
    "\n",
    "                df_dae.index.name = \"detector_method\"\n",
    "                df_dae.reset_index(inplace=True)\n",
    "                df_dae[\"method\"] = df_dae[\"detector_method\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "                df_dae[\"detector\"] = df_dae[\"detector_method\"].apply(lambda x: x.split(\"_\")[0])\n",
    "                df_dae.drop(\"detector_method\", axis=1, inplace=True)\n",
    "                df_dae = df_dae[df_dae.columns[::-1]]\n",
    "                df_dae[\"dataset\"] = dataset\n",
    "\n",
    "                df_dae_all = pd.concat([df_dae_all, df_dae], axis=0, ignore_index=True).copy()\n",
    "\n",
    "                # Calculate average MAE rate (only for ID dataset)\n",
    "                if dataset == benchmark:\n",
    "                    df_ood.drop([\"y_true\", \"y_pred_seed\"], axis=1, inplace=True)\n",
    "                    cols = [col for col in df_ood.columns if \"y_pred\" in col]\n",
    "                    df_mae = df_ood[cols].apply(lambda x: ~x).mean() * 100\n",
    "                    df_mae.name = \"mae\"\n",
    "                    df_mae = df_mae.to_frame()\n",
    "                    df_mae.index.name = \"method\"\n",
    "                    df_mae.reset_index(inplace=True)\n",
    "                    df_mae[\"method\"] = df_mae[\"method\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "\n",
    "            # reorder the columns for better inspection\n",
    "            df_fpr95_all = df_fpr95_all[[\"detector\", \"dataset\", \"method\", \"fpr95\"]].copy()\n",
    "            df_dae_all = df_dae_all[[\"detector\", \"dataset\", \"method\", \"dae_mean\", \"dae_std\"]].copy()\n",
    "            \n",
    "            # calculate the average results across all OoD datasets\n",
    "            df_fpr95_mean = df_fpr95_all.groupby([\"method\", \"detector\"]).mean().reset_index()\n",
    "            df_fpr95_mean[\"dataset\"] = \"average\"\n",
    "            df_fpr95_all = pd.concat([df_fpr95_all, df_fpr95_mean], axis=0, ignore_index=True).copy()\n",
    "            df_dae_mean = df_dae_all.groupby([\"method\", \"detector\"]).mean().reset_index()\n",
    "            df_dae_mean[\"dataset\"] = \"average\"\n",
    "            df_dae_all = pd.concat([df_dae_all, df_dae_mean], axis=0, ignore_index=True).copy()\n",
    "            \n",
    "            # sort the dataframes for better inspection\n",
    "            df_fpr95_all = df_sorted(df_fpr95_all, {\"detector\": configs[\"score_functions\"],\n",
    "                                                    \"dataset\": dataset_sorter, \"method\": method_sorter,\n",
    "                                                    })\n",
    "            df_dae_all = df_sorted(df_dae_all, {\"detector\": configs[\"score_functions\"],\n",
    "                                                \"dataset\": dataset_sorter, \"method\": method_sorter,\n",
    "                                                })\n",
    "            df_acc = df_sorted(df_acc, {\"method\": method_sorter})\n",
    "            df_mae = df_sorted(df_mae, {\"method\": method_sorter})\n",
    "\n",
    "            # save the results\n",
    "            df_acc.to_csv(os.path.join(save_dir, f\"{model_name}_{variant}_acc.csv\"), index=False)\n",
    "            df_fpr95_all.to_csv(os.path.join(save_dir, f\"{model_name}_{variant}_fpr95.csv\"), index=False)\n",
    "            df_dae_all.to_csv(os.path.join(save_dir, f\"{model_name}_{variant}_dae_rate.csv\"), index=False)\n",
    "            df_mae.to_csv(os.path.join(save_dir, f\"{model_name}_{variant}_mae.csv\"), index=False)\n",
    "\n",
    "            print(\"Results saved!\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
